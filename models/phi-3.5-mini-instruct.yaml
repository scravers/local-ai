name: phi-3.5-mini-instruct
parameters:
  model: phi-3.5-mini-instruct.gguf
  type: phi-3.5-mini-instruct
  format: gguf
backend: llama-cpp
context_size: 8192
gpu_layers: 0
threads: 4 # Ideally set this to your number of CPU cores
batch_size: 64
f16: true
download_files:
  - filename: phi-3.5-mini-instruct.gguf
    uri: https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q6_K.gguf